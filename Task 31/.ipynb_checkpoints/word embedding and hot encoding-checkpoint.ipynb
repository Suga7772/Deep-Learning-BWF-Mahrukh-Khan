{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e3b8df",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "\n",
    "Low dimensional floating-point vectors\n",
    "\n",
    "Another popular and powerful way to associate a vector with a word is the use of dense word vectors, also called word embeddings. Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445fb032",
   "metadata": {},
   "source": [
    "Vectorizing text is the process of transforming text\n",
    "into numeric tensors. This can be done in multiple ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e034958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "dimensionality = 1000\n",
    "max_length = 10\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305c464",
   "metadata": {},
   "source": [
    "### using keras for word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f297713",
   "metadata": {},
   "source": [
    "A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use\n",
    "when the number of unique tokens in your vocabulary is too large to handle explicitly.\n",
    "Instead of explicitly assigning an index to each word and keeping a reference of these\n",
    "indices in a dictionary, you can hash words into vectors of fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916c0d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650dbf5b",
   "metadata": {},
   "source": [
    "### Character-level one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64cc2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "     for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bd8b7",
   "metadata": {},
   "source": [
    "One-hot encoding is the most common, most basic way to turn a token into a vector. It consists of associating a unique integer index with every word\n",
    "and then turning this integer index i into a binary vector of size N (the size of the\n",
    "vocabulary); the vector is all zeros except for the ith entry, which is 1.  one-hot encoding can be done at the character level, as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
